This section details experimental configuration including cloud infrastructure specifications, test dataset composition, configuration matrix, and execution protocols.

\subsection{Cloud Infrastructure}

Both platforms use standardized 4 vCPU / 8GB RAM instances with identical Docker images. Table~\ref{tab:infrastructure} summarizes platform-specific configuration.

\begin{table}[h]
\centering
\caption{Cloud Infrastructure Configuration}
\label{tab:infrastructure}
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{AWS} & \textbf{Azure} \\
\midrule
Service & ECS (EC2 launch type) & Container Apps \\
Instance Type & t3.xlarge & Standard \\
vCPUs / RAM & 4 vCPUs / 8GB & 4 vCPUs / 8GB \\
Region & us-east-1 & East US \\
Load Balancer & ALB & Built-in (internal) \\
Storage & 30GB EBS gp3 & Ephemeral \\
Health Check & /api/health (30s) & /api/health (default) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Test Dataset}

The dataset contains 300 unique images from Pexels: 100 medium-resolution (12MP) and 200 high-resolution (24MP) in JPEG format. This two-tier structure simulates realistic workloads with varying computational complexity. Content distribution includes indoor scenes (30\%), outdoor nature (25\%), urban environments (20\%), people (15\%), and objects (10\%), ensuring varied inference complexity across different scene types. All images are unique, ensuring independent observations.

\subsection{Experimental Configurations}

We evaluate 12 configurations from a 2×2×3 factorial design: Platform (AWS, Azure) × Scaling Approach (Worker, Horizontal) × Scale Level (1, 2, 4). All configurations use standardized 4 vCPU / 8GB RAM instances. Table~\ref{tab:configs} summarizes the complete matrix.

\begin{table}[h]
\centering
\caption{Experimental Configuration Matrix}
\label{tab:configs}
\begin{tabular}{lccccc}
\toprule
\textbf{ID} & \textbf{Platform} & \textbf{Approach} & \textbf{Inst.} & \textbf{W/Inst} & \textbf{Total W} \\
\midrule
\multicolumn{6}{c}{\textit{Worker Scaling}} \\
\midrule
WS1 & AWS & Worker & 1 & 1 & 1 \\
WS2 & AWS & Worker & 1 & 2 & 2 \\
WS3 & AWS & Worker & 1 & 4 & 4 \\
WS4 & Azure & Worker & 1 & 1 & 1 \\
WS5 & Azure & Worker & 1 & 2 & 2 \\
WS6 & Azure & Worker & 1 & 4 & 4 \\
\midrule
\multicolumn{6}{c}{\textit{Horizontal Scaling}} \\
\midrule
HS1 & AWS & Horizontal & 1 & 1 & 1 \\
HS2 & AWS & Horizontal & 2 & 1 & 2 \\
HS3 & AWS & Horizontal & 4 & 1 & 4 \\
HS4 & Azure & Horizontal & 1 & 1 & 1 \\
HS5 & Azure & Horizontal & 2 & 1 & 2 \\
HS6 & Azure & Horizontal & 4 & 1 & 4 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Worker Scaling} tests process-level parallelism on a single instance. We evaluate 1, 2, and 4 workers on a single instance. Multiple Gunicorn workers share CPU, memory, and cache resources on the same machine.

\textbf{Horizontal Scaling} tests instance-level parallelism with load balancing. We evaluate 1, 2, and 4 instances, each running a single worker. Requests are distributed across instances via AWS ALB or Azure Container Apps' built-in load balancing.

\subsection{Execution Protocol}

Each configuration is tested 3 times (N=900 observations per configuration: 300 requests × 3 repetitions). For each run, we deploy the container with target configuration, wait 2 minutes for initialization, and execute 10 warm-up requests. The frontend loads 300 images, configures the appropriate endpoint (direct instance or load balancer), and submits all requests concurrently while capturing timing metrics. After completion, we verify no anomalies occurred, collect logs, and archive results. A 10-minute cooldown period separates runs to ensure system stabilization.

\subsection{Metrics Collection}

We collect performance and resource metrics to enable comprehensive analysis. Table~\ref{tab:metrics} summarizes all captured metrics.

\begin{table}[h]
\centering
\caption{Collected Metrics}
\label{tab:metrics}
\begin{tabular}{llp{5.5cm}}
\toprule
\textbf{Category} & \textbf{Metric} & \textbf{Description} \\
\midrule
\multirow{4}{*}{Performance}
& Total Time & Wall-clock first to last request \\
& Latency & Per-request response time \\
& Throughput & Requests/second (300 / Total) \\
& Success Rate & \% requests without HTTP errors \\
\midrule
\multirow{3}{*}{Resource}
& CPU Util. & \% sampled every 5s \\
& Memory & MB consumption \\
& Queue Latency & Semaphore wait time \\
\midrule
\multirow{2}{*}{Horizontal}
& Load Dist. & Requests per instance \\
& Balance Var. & Coefficient of variation \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Data Validation and Cost Tracking}

Before analysis, we validate that all 300 requests completed successfully with non-empty captions, monotonic timestamps, and no resource saturation. For horizontal scaling, we verify even load distribution across instances. Failed runs are discarded and re-executed with clear documentation.

Cost tracking includes instance-hour pricing multiplied by experiment duration. This enables cost-performance analysis for deployment decision-making, addressing Research Question RQ4.
