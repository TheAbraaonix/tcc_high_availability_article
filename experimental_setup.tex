This section details experimental configuration including cloud infrastructure specifications, test dataset composition, configuration matrix, and execution protocols.

\subsection{Cloud Infrastructure}

Both platforms use standardized instance configurations with identical Docker images. We initially planned to use AWS us-east-2 (Ohio) but encountered Fargate On-Demand vCPU resource limits (12 vCPUs) that prevented deploying 4 replicas simultaneously for horizontal scaling tests. Switching to AWS us-east-1 (Virginia) not only resolved this limitation but improved experimental validity: both AWS us-east-1 and Azure East US 2 data centers are physically located in Virginia, minimizing geographic distance variability and ensuring that network latency differences reflect platform architecture rather than physical proximity to the test client. Table~\ref{tab:infrastructure} summarizes platform-specific configuration.

\subsection{Test Dataset}

The dataset contains 300 unique images from Pexels~\cite{pexels}: 100 medium-resolution (12MP) and 200 high-resolution (24MP) in JPG format. This two-tier structure simulates realistic workloads with varying computational complexity. Images were collected using the Pexels API with a generic "random" search term, ensuring diverse content and varied inference complexity across different scene types. All images are unique, ensuring independent observations.

\subsection{Experimental Configurations}

We evaluate 10 configurations testing two scaling approaches independently across both platforms. Each platform tests a baseline configuration (1 worker, 1 replica) plus two scaling strategies: worker scaling (varying workers while keeping 1 replica) and horizontal scaling (varying replicas while keeping 1 worker). All configurations use standardized 4 vCPU / 8GB RAM instances. Table~\ref{tab:configs} summarizes all test configurations.

\textbf{Baseline} establishes reference performance with minimal configuration (1 replica, 1 worker), providing a control for evaluating scaling effectiveness.

\textbf{Worker Scaling} tests process-level parallelism on a single replica. We evaluate 2 and 4 workers on a single instance. Multiple Gunicorn workers share CPU, memory, and cache resources on the same machine.

\textbf{Horizontal Scaling} tests instance-level parallelism with load balancing. We evaluate 2 and 4 replicas, each running a single worker. Requests are distributed across replicas via AWS ALB or Azure Container Apps' built-in load balancing.

These approaches are tested independently (no cross-combinations) to isolate the performance impact of each scaling strategy.

\subsection{Execution Protocol}

Each configuration is tested 3 times (N=900 observations per configuration: 300 requests Ã— 3 repetitions). For each run, we deploy the container with target configuration, wait 2 minutes for initialization, and execute 30 warm-up requests. The frontend loads 300 images, configures the appropriate endpoint (direct instance or load balancer), and submits all requests concurrently while capturing timing metrics. After completion, we verify no anomalies occurred, collect logs, and archive results. A 10-minute cooldown period separates runs to ensure system stabilization.

\subsection{Metrics Collection}

We collect performance metrics to enable comprehensive analysis. Table~\ref{tab:metrics} summarizes all captured metrics.

\subsection{Data Validation}

Before analysis, we validate that all 300 requests per run completed successfully with valid HTTP responses. Each configuration is tested 3 times, and metrics are averaged across the 3 repetitions to ensure reliability. Runs with anomalies or incomplete data are discarded and re-executed.

\subsection{Cost Analysis}

To address Research Question RQ4, we perform cost-performance analysis based on published AWS and Azure pricing models~\cite{aws2023fargate_pricing, aws2023elb_pricing, aws2023ecr_pricing, azure2023containerapps_pricing, azure2023acr_pricing}. Cost calculation considers instance pricing (based on vCPU/memory allocation), load balancer costs (for horizontal scaling configurations), and total experiment duration. This enables comparison of cost efficiency across different scaling strategies and platforms, providing practical guidance for deployment decisions. All pricing is based on on-demand rates for the us-east-1 (AWS) and East US 2 (Azure) regions as of the experiment date.

\subsection{Experimental Setup Tables}

The following tables provide detailed specifications for the experimental configuration.

\begin{table*}[!t]
\centering
\caption{Cloud Infrastructure Configuration}
\label{tab:infrastructure}
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{AWS} & \textbf{Azure} \\
\midrule
Service & ECS (Fargate launch type) & Container Apps \\
Compute Allocation & 4 vCPUs / 8GB & 4 vCPUs / 8GB \\
Region & us-east-1 (Virginia) &  East US 2 (Virginia) \\
Load Balancer & ALB & Built-in (internal) \\
Health Check & /api/health (30s) & /api/health (default probe) \\
Networking & AWS VPC + Private Subnets & Virtual Network (automatic) \\
Logging/Monitoring & CloudWatch Logs & Azure Monitor \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[!t]
\centering
\caption{Test Configurations}
\label{tab:configs}
\begin{tabular}{lcccc}
\toprule
\textbf{ID} & \textbf{Platform} & \textbf{Approach} & \textbf{Replicas} & \textbf{Workers} \\
\midrule
\multicolumn{5}{c}{\textit{Baseline}} \\
\midrule
B1 & AWS & Baseline & 1 & 1 \\
B2 & Azure & Baseline & 1 & 1 \\
\midrule
\multicolumn{5}{c}{\textit{Worker Scaling}} \\
\midrule
WS1 & AWS & Worker & 1 & 2 \\
WS2 & AWS & Worker & 1 & 4 \\
WS3 & Azure & Worker & 1 & 2 \\
WS4 & Azure & Worker & 1 & 4 \\
\midrule
\multicolumn{5}{c}{\textit{Horizontal Scaling}} \\
\midrule
HS1 & AWS & Horizontal & 2 & 1 \\
HS2 & AWS & Horizontal & 4 & 1 \\
HS3 & Azure & Horizontal & 2 & 1 \\
HS4 & Azure & Horizontal & 4 & 1 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[!t]
\centering
\caption{Collected Metrics}
\label{tab:metrics}
\begin{tabular}{lp{8cm}}
\toprule
\textbf{Metric} & \textbf{Description} \\
\midrule
Avg Response Time & Mean response time across all requests \\
Max Response Time & Maximum response time observed \\
Min Response Time & Minimum response time observed \\
Median Response Time & Median (50th percentile) response time \\
Std Deviation & Standard deviation of response times \\
Failed Requests & Count of requests with HTTP errors \\
Success Rate & Percentage of requests completed without errors \\
\bottomrule
\end{tabular}
\end{table*}

\FloatBarrier