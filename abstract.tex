Cloud platform selection for deploying CPU-intensive AI inference services requires empirical performance evidence, yet systematic comparisons between major providers remain limited. This study presents a controlled experimental comparison of Amazon Elastic Container Service (Amazon ECS) with EC2 launch type and Azure Container Apps for CPU-bound image captioning workloads, evaluating two distinct scaling approaches: worker scaling (multiple processes on a single instance) and horizontal scaling (multiple instances with load balancing).

Using standardized 4 vCPU / 8GB RAM instances, we deploy a FastAPI-based BLIP image captioning service and process 300 concurrent requests (100 medium-resolution 12MP and 200 high-resolution 24MP images from Pexels) across 12 configurations. Each configuration is tested 3 times on both platforms to ensure statistical reliability (N=900 observations per configuration). We address critical implementation challenges including CPU thread oversubscription, Python Global Interpreter Lock limitations, and load balancer configuration for horizontal scaling.

Results reveal [FILL: platform-specific performance characteristics]. AWS demonstrates [FILL]\% [higher/lower] throughput than Azure, with [FILL]\% difference in latency variance. Both platforms exhibit sub-linear scaling behavior, with worker scaling achieving [FILL]\% efficiency at 4 workers and horizontal scaling achieving [FILL]\% efficiency at 4 instances. Cost-performance analysis shows [FILL: optimal configuration] provides best value for [FILL: specific use case]. These findings provide empirical evidence for platform selection decisions in CPU-intensive AI inference deployments, quantifying performance and cost trade-offs between scaling approaches across major cloud providers.
