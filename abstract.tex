Cloud platform selection for deploying CPU-intensive AI inference services requires empirical performance evidence under controlled experimental conditions, yet systematic comparisons between major providers using identical hardware specifications remain limited. This study presents a controlled experimental comparison of Amazon Elastic Container Service with Fargate launch type and Azure Container Apps for CPU-bound image captioning workloads, evaluating worker scaling (multiple processes on single instance) versus horizontal scaling (multiple instances with load balancing). Using standardized 4 vCPU / 8 gigabyte RAM instances, we deploy a BLIP image captioning service processing 300 concurrent requests across 10 configurations with 3 repetitions per configuration (N=900 observations), addressing implementation challenges including CPU thread oversubscription, Python Global Interpreter Lock limitations, and load balancer configuration complexity. Results reveal dramatic platform-specific scaling divergence: AWS achieves exceptional horizontal scaling efficiency (3.42× speedup at 4 replicas) while Azure plateaus at 1.50× maximum speedup, representing a 128 percent efficiency difference. Worker scaling exhibits similar performance across platforms (AWS 1.69× versus Azure 1.45×). Overall, AWS demonstrates 1.9 percent lower average response time while Azure provides 7.1 percent lower variance. Cost analysis reveals Azure costs 4.7-5.4× more than AWS for equivalent resources, yet AWS delivers 94 percent better performance at optimal configuration. Platform-specific optimal strategies differ markedly: AWS horizontal scaling vastly outperforms worker scaling while Azure exhibits equivalent performance for both approaches. These findings demonstrate that scaling approach selection proves more consequential than platform choice alone, with configuration decisions capable of tripling performance. This work provides empirical evidence and platform-specific deployment strategies.
