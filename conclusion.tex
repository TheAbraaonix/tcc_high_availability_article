This study presented a controlled experimental comparison of Amazon ECS with Fargate launch type and Azure Container Apps for CPU-intensive AI inference workloads, evaluating worker scaling versus horizontal scaling approaches under identical hardware (4 vCPU / 8GB RAM) and workload conditions (300 concurrent requests). Our systematic evaluation across 10 configurations with 3 repetitions each (N=900 observations per configuration) provides empirical evidence for platform selection and scaling configuration decisions.

\subsection{Key Findings}

\textbf{RQ1: Platform Performance Comparison}. When averaged across all configurations, AWS demonstrates marginally superior performance with 1.9\% lower average response time (172.77s vs 176.03s). However, Azure provides 7.1\% lower response time variance (89.56s vs 96.37s standard deviation), indicating better overall predictability across configurations. The critical finding is that platform choice has minimal impact on average performance; instead, scaling strategy selection proves far more consequential, as evidenced by dramatic performance variations within each platform (AWS: 79.72s to 272.77s; Azure: 154.81s to 231.78s). Both platforms achieved 100\% success rates across all configurations, validating deployment robustness.

\textbf{RQ2: Scaling Behavior Divergence}. AWS and Azure exhibit strikingly different scaling behavior depending on approach. For worker scaling (multiple processes on single instance), platforms perform similarly: AWS achieves 1.69× speedup with 4 workers while Azure achieves 1.45× speedup; only 16.6\% difference. Both platforms demonstrate sub-linear scaling due to CPU contention, cache interference, and memory bandwidth saturation. For horizontal scaling (multiple instances with load balancing), however, platforms diverge dramatically: AWS achieves exceptional 3.42× speedup at 4 replicas (85.5\% efficiency), approaching perfect linear scaling, while Azure achieves only 1.50× speedup with performance plateauing beyond 2 replicas. This 128\% scaling efficiency difference represents fundamentally different architectural capabilities, likely stemming from AWS Application Load Balancer's efficient request distribution versus Azure Container Apps' built-in load balancer bottlenecks.

\textbf{RQ3: Performance Consistency and Predictability}. Azure provides better overall consistency with 7.1\% lower response time variance across all configurations (89.56s vs 96.37s standard deviation). However, the consistency winner depends on scaling approach: for horizontal scaling, AWS demonstrates dramatic variance reduction (151.69s → 43.13s, 71.6\% reduction at 4 replicas) while Azure shows modest improvement (124.09s → 62.88s, 49.3\% reduction). AWS's horizontal scaling configuration (HS2: 79.72s ± 43.13s) provides both the fastest response time and lowest variance simultaneously, making it optimal for latency-sensitive applications requiring predictable performance. Both platforms show improved consistency as worker count increases, suggesting better load distribution across processes mitigates variance despite resource contention.

\textbf{RQ4: Cost-Performance Trade-Offs}. Azure Container Apps costs 4.7-5.4× more than AWS ECS with Fargate for equivalent resource allocations. At 4 replicas, Azure costs \$1.735 per hour versus AWS's \$0.345 per hour (including load balancer charges), yet delivers significantly worse performance (154.81s vs 79.72s). AWS horizontal scaling (HS2) provides best average response time (79.72s) and lowest variance (±43.13s) at \$0.345 per hour, delivering 94\% better performance than the best Azure configuration at one-fifth the cost. For budget-constrained deployments, AWS worker scaling configurations provide lowest hourly cost (\$0.081 per hour) with moderate performance (161.46s for 4 workers). Azure remains viable only for simpler deployments where built-in load balancing eliminates ALB configuration complexity, accepting both higher costs and performance limitations. The optimal configuration depends on priorities: AWS HS2 for latency-sensitive workloads, AWS worker scaling for cost optimization, Azure worker scaling for deployment simplicity with acceptable performance.

\subsection{Implementation Challenges and Solutions}

Deploying multi-worker Python-based AI inference services revealed critical implementation challenges requiring careful design decisions. CPU thread oversubscription occurs when numerical libraries (NumPy, PyTorch) spawn thread pools at import time based on total system vCPUs, unaware of other worker processes. Without explicit thread limiting, 4 workers on a 4 vCPU instance create 16 threads competing for 4 vCPUs, causing catastrophic performance degradation. Our dynamic thread limiting strategy prevents oversubscription by configuring environment variables before library imports to divide available vCPUs across worker processes. Python's Global Interpreter Lock creates complete blocking behavior in single-worker deployments; concurrent requests execute strictly sequentially regardless of FastAPI's asynchronous capabilities. Multi-worker deployment via Gunicorn circumvents this limitation through process-level parallelism. AWS's explicit Application Load Balancer configuration requires manual creation of Target Groups, Listeners, and port mappings with precise coordination; port misconfigurations cause silent failures. Azure Container Apps abstracts this complexity entirely, activating load balancing automatically when scaling to multiple replicas. These solutions generalize to any CPU-intensive Python deployment with multi-worker configurations.

\subsection{Practical Contributions}

This work provides empirical evidence enabling practitioners to make informed platform selection and scaling configuration decisions for CPU-intensive AI inference deployments. Platform-specific recommendations emerge clearly: for AWS, prioritize horizontal scaling with ALB-based load balancing (3.42× speedup at 4 replicas) despite configuration complexity; for Azure, worker scaling and horizontal scaling provide equivalent performance (1.45× vs 1.50×), making worker scaling preferable due to zero load balancer configuration overhead and lower cost. Organizations should prioritize scaling approach selection over platform choice alone; configuration decisions can triple performance (AWS baseline: 272.77s → 79.72s at 4 replicas). Our reproducible experimental framework (complete system implementation, Docker containers, experimental protocols, thread limiting configuration) enables replication and extension to other AI models, cloud platforms, and workload patterns. The controlled comparison methodology eliminates hardware variability as a confounding variable, providing clear attribution of performance differences to platform architecture and scaling approach.

\subsection{Limitations}

This study focuses on BLIP image captioning (medium-sized transformer model) and CPU-only inference. Larger models, GPU-accelerated inference, or different neural network architectures may exhibit different scaling characteristics due to distinct computational bottlenecks (GPU memory bandwidth, PCIe transfer overhead, CUDA kernel launch latency). Our concurrent batch submission (300 simultaneous requests) represents a stress-test scenario; real-world workloads with variable arrival rates following Poisson or bursty distributions may favor different configurations. The study evaluates AWS and Azure exclusively; other major cloud providers (Google Cloud Platform, Oracle Cloud Infrastructure) may demonstrate different performance profiles and load balancing architectures. We did not collect detailed resource utilization metrics (CPU percentage, memory consumption, network I/O) which could provide deeper insights into platform-level architectural differences and bottleneck identification.

\subsection{Future Work}

Future research should extend this comparison to GPU-accelerated inference, where fundamentally different bottlenecks (PCIe bandwidth for host-device transfers, GPU memory capacity, CUDA kernel scheduling) may alter scaling behavior and platform performance characteristics. Evaluating additional cloud providers; particularly Google Cloud Run, Google Kubernetes Engine, Oracle Container Instances, and IBM Cloud Code Engine; would broaden platform comparison scope and identify whether AWS's horizontal scaling advantage generalizes across providers or represents AWS-specific architectural strength. Testing under realistic request arrival distributions (Poisson processes, diurnal patterns, bursty traffic) rather than concurrent batch submission would validate findings for production workloads and inform auto-scaling policy design. Investigating dynamic auto-scaling policies based on queue depth, response time percentiles, and predictive workload forecasting could optimize resource utilization for variable workloads while maintaining service-level objectives. Finally, extending the experimental framework to containerized serving systems (TorchServe, TensorFlow Serving, NVIDIA Triton Inference Server) would evaluate whether platform-specific scaling characteristics persist across different serving frameworks or represent FastAPI/Gunicorn-specific behavior.
