This section defines fundamental concepts required to understand our experimental comparison of AWS and Azure cloud platforms under different scaling approaches.

\subsection{Cloud Platforms and Virtualization}

Cloud computing platforms provide virtualized computational resources. Amazon ECS (Elastic Container Service) with Fargate launch type and Azure Container Apps abstract physical hardware into virtual CPUs (vCPUs) and memory allocations~\cite{aws2023ha, azure2023ha, aws2023ec2}. In hyperthreaded architectures, one vCPU equals one hardware thread (half a physical core)~\cite{intel2023ht}. Thread oversubscription occurs when total threads exceed available vCPUs, causing context-switching overhead and performance degradation~\cite{intel2023mkl}.

\subsection{Scaling Approaches}

\textbf{Worker Scaling} (process-level parallelism): Multiple worker processes execute on a single instance, sharing CPU, memory, and cache resources. Application servers such as Gunicorn spawn independent worker processes~\cite{gunicorn2023docs}. This approach minimizes infrastructure cost but introduces resource contention as worker count increases.

\textbf{Horizontal Scaling} (instance-level parallelism): Multiple instances run independently with load balancers distributing incoming requests across instances. Both AWS Application Load Balancer (ALB) and Azure Container Apps use round-robin distribution to ensure even load across replicas~\cite{aws2023elb, chieu2010dynamic}. ALB provides explicit health checking and traffic distribution configuration, while Azure Container Apps manages load balancing internally when scaling to multiple replicas. This approach provides better fault isolation and potentially more predictable performance, but incurs additional infrastructure costs (and explicit load balancer costs for AWS).

\subsection{Python Global Interpreter Lock and Thread Management}

The Global Interpreter Lock (GIL) prevents parallel execution of Python bytecode within a single process~\cite{python2023gil}. For CPU-bound tasks, multi-process parallelism (worker scaling) circumvents this limitation by creating independent Python processes, each with its own GIL. C-extensions such as NumPy, PyTorch, and TensorFlow release the GIL during computationally intensive operations~\cite{pytorch2023threading}, enabling thread-level parallelism within individual workers.

However, these numerical libraries create internal thread pools at import time based on detecting total system vCPUs, not per-worker allocations. Without explicit thread limiting, deploying multiple workers causes thread oversubscription: each worker spawns a thread pool sized for the entire system (e.g., 4 threads per worker on a 4 vCPU instance), resulting in total threads far exceeding available vCPUs (e.g., 4 workers Ã— 4 threads = 16 threads competing for 4 vCPUs). This oversubscription causes excessive context switching and cache contention, degrading performance below single-worker baselines. Thread pool sizes must be configured via environment variables before library import, as they become fixed once the libraries initialize~\cite{intel2023mkl, pytorch2023threading}.
