Cloud computing adoption continues accelerating, with organizations increasingly deploying AI inference services on platforms such as Amazon Web Services (AWS) and Microsoft Azure~\cite{aws2023ha, azure2023ha}. Platform selection and scaling configuration decisions directly impact response time performance, operational costs, and deployment complexity. However, empirical comparisons between major cloud providers remain limited, particularly for CPU-intensive AI inference workloads under controlled experimental conditions with identical hardware specifications. Practitioners face critical deployment decisions; which platform to select and how to scale (multiple workers per instance versus multiple instances with load balancing); without sufficient empirical evidence quantifying performance, cost, and consistency trade-offs.

Existing cloud performance studies primarily evaluate synthetic benchmarks or general-purpose web applications~\cite{li2013survey, chieu2010dynamic}, with insufficient focus on CPU-bound AI inference workloads exhibiting unique characteristics: blocking inference operations (2-5 seconds per request), Python Global Interpreter Lock limitations preventing intra-process parallelism, and numerical library thread management challenges. Furthermore, existing comparisons typically use heterogeneous hardware configurations across platforms, making it impossible to isolate platform-level architectural differences from instance type variations. Scaling approach decisions; whether to deploy multiple worker processes on a single instance (worker scaling) or multiple instances behind a load balancer (horizontal scaling); lack empirical guidance for CPU-intensive scenarios where inference operations monopolize worker processes.

Our systematic mapping study~\cite{holandabilharinhodejesus2025high} of 706 papers revealed a fragmented research landscape with only 5 publications directly addressing high availability strategies for AI systems in cloud environments. None of these studies conducted controlled cross-platform performance comparisons with identical hardware specifications, nor did they systematically evaluate scaling approach trade-offs for CPU-intensive AI inference workloads. Implementation challenges encountered when deploying multi-worker Python-based AI services; including CPU thread oversubscription when numerical libraries spawn excessive threads, GIL-induced blocking preventing concurrent request handling, and platform-specific load balancer configuration complexity; remain poorly documented in academic literature.

\subsection{Research Questions}

This study addresses the following research questions:

\begin{itemize}
\item \textbf{RQ1}: How do AWS and Azure compare in response time performance and consistency for CPU-intensive AI inference workloads?
\item \textbf{RQ2}: Do AWS and Azure exhibit different scaling behavior under worker scaling versus horizontal scaling approaches?
\item \textbf{RQ3}: Which platform provides better performance consistency and predictability across different scaling configurations?
\item \textbf{RQ4}: What are the cost-performance trade-offs between AWS and Azure for different scaling strategies?
\end{itemize}

\subsection{Contributions}

This work makes the following contributions:

\textbf{(1) Discovery of Dramatic Platform-Specific Scaling Divergence}: Through controlled experimentation across 10 configurations with identical 4 vCPU / 8GB RAM instances, we reveal that AWS achieves exceptional horizontal scaling efficiency (3.42× speedup at 4 replicas, approaching perfect linear scaling) while Azure plateaus beyond 2 replicas (1.50× maximum speedup); a 128\% efficiency difference representing fundamentally different load balancing architectures. Worker scaling exhibits similar performance across platforms (AWS 1.69× vs Azure 1.45×), demonstrating that architectural differences manifest primarily in distributed replica scenarios.

\textbf{(2) Quantification of Cost-Performance Disparity}: We establish that Azure Container Apps costs 4.7-5.4× more than AWS ECS with Fargate for equivalent resource allocations, yet delivers significantly worse performance at scale. AWS horizontal scaling (4 replicas: 79.72s at \$0.345/hour) provides 94\% better performance than the best Azure configuration (154.81s at \$1.735/hour) at one-fifth the hourly cost, identifying clear cost-performance optimization strategies for production deployments.

\textbf{(3) Platform-Specific Deployment Strategies}: Our empirical evidence demonstrates that optimal scaling approaches are platform-dependent. For AWS, horizontal scaling with Application Load Balancer provides vastly superior efficiency (3.42× vs 1.69× for worker scaling), justifying ALB configuration complexity. For Azure, worker scaling and horizontal scaling achieve equivalent performance (1.45× vs 1.50×), making worker scaling preferable due to zero load balancer configuration overhead and lower infrastructure costs. These findings challenge the assumption that horizontal scaling universally outperforms worker scaling.

\textbf{(4) Documentation of Implementation Challenges and Solutions}: We systematically document critical challenges encountered in multi-worker Python AI deployments; CPU thread oversubscription causing 4 workers to spawn 16 threads on 4 vCPUs, GIL-induced blocking forcing sequential request processing in single-worker configurations, and AWS ALB port mapping coordination requirements. Our solutions (dynamic thread limiting via environment variables, multi-worker Gunicorn~\cite{gunicorn2023docs} deployment, semaphore-based concurrency control, platform-specific load balancer configuration) generalize to any CPU-intensive Python deployment with numerical libraries.

\textbf{(5) Reproducible Experimental Framework}: We provide complete system implementation including Docker~\cite{docker2024} containers, experimental protocols, thread limiting configurations, and controlled comparison methodology. The framework eliminates hardware variability as a confounding variable, enabling clear attribution of performance differences to platform architecture and scaling approach. All configurations enable replication and extension to other AI models, cloud platforms, and workload patterns.

\subsection{Scope}

This study evaluates cloud platform performance for CPU-intensive AI inference workloads through systematic experimentation across 10 configurations with 3 repetitions each (N=900 observations per configuration). We use BLIP image captioning~\cite{li2022blip} as a representative CPU-bound computational task providing consistent, measurable demand ideal for controlled comparison; the AI aspect is incidental to infrastructure performance evaluation. Our focus is platform architecture: comparing Amazon ECS with Fargate launch type versus Azure Container Apps across worker scaling (1, 2, 4 workers on single instance) and horizontal scaling (1, 2, 4 instances with load balancing) approaches.

All experiments use standardized 4 vCPU / 8GB RAM instances on both platforms to eliminate instance size as a confounding variable. This controlled approach isolates the performance impact of platform selection and scaling configuration from hardware variability. Experiments process 300 concurrent requests (100 medium-resolution 12MP and 200 high-resolution 24MP images from Pexels) submitted simultaneously to create maximum concurrency stress, measuring response time, variance, success rate, and cost metrics.

To measure pure inference performance without network download contamination, the frontend pre-loads all images into browser memory and submits them as multipart/form-data file uploads. This ensures image download time (which can exceed inference time for large images) does not contaminate performance measurements or introduce bottlenecks unrelated to cloud platform performance.

\subsection{Key Findings Preview}

Our experiments reveal striking platform-specific scaling characteristics with significant practical implications. AWS achieves near-linear horizontal scaling (3.42× speedup at 4 replicas, 85.5\% efficiency) while Azure performance plateaus beyond 2 replicas (1.50× maximum speedup regardless of replica count), representing fundamentally different load balancing capabilities. Despite Azure costing 4.7-5.4× more per hour, AWS delivers 94\% better performance at the optimal configuration. The most surprising finding is that scaling approach selection proves more consequential than platform choice alone; within AWS, configuration decisions can triple performance (baseline 272.77s → 79.72s at 4 replicas horizontal scaling). These findings demonstrate that practitioners must account for platform-specific architectural characteristics when selecting scaling strategies, challenging conventional wisdom that horizontal scaling universally outperforms worker scaling across all cloud providers.

\subsection{Paper Organization}

The remainder of this paper is organized as follows. Section 2 provides essential background on cloud platforms, scaling approaches, and Python Global Interpreter Lock. Section 3 reviews related work on cloud platform comparisons, AI model serving, and scaling strategies, positioning our contributions within the research landscape. Section 4 describes system architecture, implementation challenges (thread oversubscription, GIL blocking, load balancer configuration), and design decisions. Section 5 details experimental setup including platform configurations, test dataset composition, execution protocols, and metrics collection. Section 6 presents results and discusses findings, directly answering research questions through platform comparison and scaling behavior analysis. Section 7 concludes with key findings, practical contributions, limitations, and future work directions.
