Cloud computing platforms such as Amazon Web Services (AWS) and Microsoft Azure provide Infrastructure-as-a-Service (IaaS) solutions for deploying AI inference services~\cite{aws2023ha, azure2023ha}. Organizations deploying CPU-intensive AI workloads face critical platform selection and scaling configuration decisions that directly impact performance, cost, and operational efficiency. However, empirical comparisons between major cloud providers remain limited, particularly for CPU-bound AI inference workloads under controlled experimental conditions.

Platform selection typically relies on vendor documentation, pricing calculators, and anecdotal evidence rather than rigorous experimental comparison. Existing cloud performance studies~\cite{li2013survey, chieu2010dynamic} primarily evaluate synthetic benchmarks or general-purpose applications, with insufficient focus on CPU-intensive AI inference workloads. Furthermore, scaling approach decisions—whether to deploy multiple worker processes on a single instance (worker scaling) or multiple instances behind a load balancer (horizontal scaling)—lack empirical guidance for CPU-bound scenarios.

Deploying multi-worker Python-based AI services introduces critical implementation challenges. CPU-bound inference operations create complete blocking behavior in single-worker deployments: concurrent requests execute strictly sequentially (FIFO) regardless of FastAPI's asynchronous capabilities, making default configurations unusable for production workloads. Additional challenges include CPU thread oversubscription when multiple workers spawn excessive threads, Python Global Interpreter Lock (GIL) limitations~\cite{python2023gil} preventing intra-process parallelism, and resource contention across worker processes. These challenges necessitate manual worker and thread management rather than relying on framework defaults. Understanding these challenges and their solutions provides practical value beyond performance measurements alone.

\subsection{Research Questions}

This study addresses the following research questions:

\begin{itemize}
\item \textbf{RQ1}: How do AWS and Azure compare in throughput, latency, and resource utilization for CPU-intensive AI inference workloads?
\item \textbf{RQ2}: Do AWS and Azure exhibit different scaling behavior under worker scaling versus horizontal scaling approaches?
\item \textbf{RQ3}: Which platform provides better performance consistency and predictability across different scaling configurations?
\item \textbf{RQ4}: What are the cost-performance trade-offs between AWS and Azure for different scaling strategies?
\end{itemize}

\subsection{Contributions}

This work makes the following contributions:

\textbf{(1) Controlled Cross-Platform Comparison}: We provide empirical performance comparison of Amazon ECS with EC2 launch type and Azure Container Apps under identical hardware specifications (4 vCPU / 8GB RAM), workload conditions (300 concurrent requests), and software configurations, enabling fair platform comparison.

\textbf{(2) Scaling Approach Evaluation}: We systematically compare worker scaling (1, 2, 4 workers on single instance) versus horizontal scaling (1, 2, 4 instances with load balancing) across both platforms, quantifying performance and cost trade-offs.

\textbf{(3) Implementation Challenges and Solutions}: We document critical challenges encountered when deploying multi-worker Python-based AI services and present design decisions that address CPU thread oversubscription, GIL limitations, and load balancer configuration.

\textbf{(4) Reproducible Experimental Framework}: We provide complete system implementation, experimental protocols, and configurations enabling replication and extension to other AI models and cloud platforms.

\textbf{(5) Practical Deployment Guidance}: We translate experimental findings into actionable deployment recommendations, helping practitioners select platforms and scaling configurations based on performance requirements, cost constraints, and predictability needs.

\subsection{Scope}

This study evaluates cloud platform performance for CPU-intensive AI inference workloads. We use BLIP image captioning~\cite{baylor2017tfserving} as a representative computational task because it provides consistent, measurable CPU demand ideal for controlled comparison—the AI aspect is incidental to the performance evaluation. Our focus is infrastructure performance: comparing AWS versus Azure across worker and horizontal scaling approaches.

All experiments use standardized 4 vCPU / 8GB RAM instances to eliminate instance size as a confounding variable. This controlled approach isolates the performance impact of platform selection and scaling configuration from hardware variability. Experiments process 300 concurrent requests (100 medium-resolution 12MP and 200 high-resolution 24MP images from Pexels) with 3 repetitions per configuration to ensure statistical reliability.

To measure pure inference performance without network download contamination, the frontend pre-loads all images and submits them as file uploads. This ensures image download time (which can exceed inference time) does not contaminate performance measurements or introduce bottlenecks unrelated to cloud platform performance.

\subsection{Paper Organization}

The remainder of this paper is organized as follows. Section 2 provides essential background on cloud platforms, scaling approaches, and the Python GIL. Section 3 describes system architecture, implementation challenges, and design decisions. Section 4 details experimental setup including platform configurations, test dataset, and execution protocols. Section 5 presents results and discusses findings, answering research questions and comparing platforms. Section 6 concludes with key findings and future work directions.
